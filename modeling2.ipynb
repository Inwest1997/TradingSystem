{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.data_loader import *\n",
    "from src.index import *\n",
    "from src.ticker_selection import *\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl = DataGenerator('db', 'day').data_search('AAPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7534, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aapl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl['rsi'] = rsi(aapl)\n",
    "aapl['macd'] = macd(aapl)['macd']\n",
    "aapl[['en_center', 'en_ub', 'en_lb']] = envelope(aapl)\n",
    "aapl[['bo_center', 'bo_ub', 'bo_lb']] = bollinger(aapl)\n",
    "aapl[['slow_k', 'slow_d']] = stochastic(aapl)\n",
    "aapl['SMA(120)'] = sma(aapl,120)\n",
    "aapl['SMA(60)'] = sma(aapl,60)\n",
    "aapl['SMA(20)'] = sma(aapl,20)\n",
    "aapl['SMA(5)']= sma(aapl,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl = stock_standard(aapl).calculator(standard=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl.drop('Standard', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl.to_csv('aapl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_slice(data, target,time_steps, for_periods, split_size = 0.8): \n",
    "    \"\"\"\n",
    "    input:\n",
    "     data: 날짜를 인덱스로 가지는 주식가격(Adj Close) 데이터\n",
    "    output:\n",
    "     X_train, y_train: 2013/1/1부터 2018-12/31까지의 데이터\n",
    "     X_test : 2019년 동안의 데이터 \n",
    "    time_steps: # input 데이터의 time steps\n",
    "    for_periods: # output 데이터의 time steps\n",
    "    \"\"\"\n",
    "    X = [] \n",
    "    y = [] \n",
    "\n",
    "    for i in range(time_steps, len(data.values) - 1):\n",
    "        X.append(data.values[i-time_steps:i,:])\n",
    "        y.append(data[target].values[i:i+for_periods])\n",
    "    X, y = np.array(X), np.array(y)\n",
    "\n",
    "    X_train = Variable(torch.Tensor(X[:int(split_size * len(X))] ))\n",
    "    X_train = torch.reshape(X_train,(X_train.shape[0], X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "    X_test =  Variable(torch.Tensor(X[int(split_size * len(X)):] ))\n",
    "    X_test = torch.reshape(X_test,(X_test.shape[0], X_test.shape[1], X_test.shape[2]))\n",
    "    \n",
    "    y_train = Variable(torch.Tensor(y[:int(split_size * len(y))]))\n",
    "    y_train = torch.reshape(y_train,(y_train.shape[0],y_train.shape[1], 1 ))\n",
    "\n",
    "    y_test = Variable(torch.Tensor(y[int(split_size * len(y)):]))\n",
    "    y_test = torch.reshape(y_test,(y_test.shape[0],y_train.shape[1], 1))\n",
    "\n",
    "\n",
    "    return X_train, X_test, y_train, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test  = window_slice(aapl, 'Adj Close', 5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5927, 5, 29])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM1(nn.Module):\n",
    "  def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
    "    super(LSTM1, self).__init__()\n",
    "    self.num_classes = num_classes #number of classes\n",
    "    self.num_layers = num_layers #number of layers\n",
    "    self.input_size = input_size #input size\n",
    "    self.hidden_size = hidden_size #hidden state\n",
    "    self.seq_length = seq_length #sequence length\n",
    " \n",
    "    self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                      num_layers=num_layers, batch_first=True) #lstm\n",
    "    self.fc_1 =  nn.Linear(hidden_size, 128) #fully connected 1\n",
    "    self.fc = nn.Linear(128, num_classes) #fully connected last layer\n",
    "\n",
    "    self.relu = nn.ReLU() \n",
    "\n",
    "  def forward(self,x):\n",
    "    h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))#.to(device) #hidden state\n",
    "    c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))#.to(device) #internal state   \n",
    "    # Propagate input through LSTM\n",
    "\n",
    "    output, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "   \n",
    "    hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
    "    out = self.relu(hn)\n",
    "    out = self.fc_1(out) #first Dense\n",
    "    out = self.relu(out) #relu\n",
    "    out = self.fc(out) #Final Output\n",
    "   \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>rsi</th>\n",
       "      <th>macd</th>\n",
       "      <th>en_center</th>\n",
       "      <th>en_ub</th>\n",
       "      <th>...</th>\n",
       "      <th>SMA(5)</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>s7</th>\n",
       "      <th>s8</th>\n",
       "      <th>s9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1993-05-21</th>\n",
       "      <td>0.524554</td>\n",
       "      <td>0.527902</td>\n",
       "      <td>0.506696</td>\n",
       "      <td>0.513393</td>\n",
       "      <td>0.423102</td>\n",
       "      <td>148198400.0</td>\n",
       "      <td>64.511646</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.386439</td>\n",
       "      <td>0.405761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419055</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993-05-24</th>\n",
       "      <td>0.506696</td>\n",
       "      <td>0.524554</td>\n",
       "      <td>0.506696</td>\n",
       "      <td>0.514509</td>\n",
       "      <td>0.424022</td>\n",
       "      <td>150315200.0</td>\n",
       "      <td>64.877338</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.386642</td>\n",
       "      <td>0.405974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.421815</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993-05-25</th>\n",
       "      <td>0.506696</td>\n",
       "      <td>0.513393</td>\n",
       "      <td>0.497768</td>\n",
       "      <td>0.503348</td>\n",
       "      <td>0.414824</td>\n",
       "      <td>180723200.0</td>\n",
       "      <td>58.396195</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.386550</td>\n",
       "      <td>0.405877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993-05-26</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.515625</td>\n",
       "      <td>0.494420</td>\n",
       "      <td>0.515625</td>\n",
       "      <td>0.424942</td>\n",
       "      <td>121564800.0</td>\n",
       "      <td>62.798638</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.386734</td>\n",
       "      <td>0.406071</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993-05-27</th>\n",
       "      <td>0.515625</td>\n",
       "      <td>0.522321</td>\n",
       "      <td>0.511161</td>\n",
       "      <td>0.513393</td>\n",
       "      <td>0.423102</td>\n",
       "      <td>197288000.0</td>\n",
       "      <td>61.523974</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.387083</td>\n",
       "      <td>0.406437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.421998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-24</th>\n",
       "      <td>147.190002</td>\n",
       "      <td>150.229996</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>149.449997</td>\n",
       "      <td>149.449997</td>\n",
       "      <td>75981900.0</td>\n",
       "      <td>54.436786</td>\n",
       "      <td>-1.88</td>\n",
       "      <td>153.653999</td>\n",
       "      <td>161.336699</td>\n",
       "      <td>...</td>\n",
       "      <td>145.544000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-25</th>\n",
       "      <td>150.089996</td>\n",
       "      <td>152.490005</td>\n",
       "      <td>149.360001</td>\n",
       "      <td>152.339996</td>\n",
       "      <td>152.339996</td>\n",
       "      <td>74732290.0</td>\n",
       "      <td>58.207938</td>\n",
       "      <td>-1.12</td>\n",
       "      <td>153.236999</td>\n",
       "      <td>160.898849</td>\n",
       "      <td>...</td>\n",
       "      <td>147.262000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-25</th>\n",
       "      <td>150.089996</td>\n",
       "      <td>152.490005</td>\n",
       "      <td>149.360001</td>\n",
       "      <td>152.339996</td>\n",
       "      <td>152.339996</td>\n",
       "      <td>74732300.0</td>\n",
       "      <td>58.207938</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>152.823199</td>\n",
       "      <td>160.464359</td>\n",
       "      <td>...</td>\n",
       "      <td>148.957999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-26</th>\n",
       "      <td>150.960007</td>\n",
       "      <td>151.990005</td>\n",
       "      <td>148.039993</td>\n",
       "      <td>149.350006</td>\n",
       "      <td>149.350006</td>\n",
       "      <td>88194300.0</td>\n",
       "      <td>52.949432</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>152.319199</td>\n",
       "      <td>159.935159</td>\n",
       "      <td>...</td>\n",
       "      <td>150.150000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>148.070007</td>\n",
       "      <td>149.050003</td>\n",
       "      <td>144.130005</td>\n",
       "      <td>144.800003</td>\n",
       "      <td>144.800003</td>\n",
       "      <td>108284100.0</td>\n",
       "      <td>46.121214</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>151.732199</td>\n",
       "      <td>159.318809</td>\n",
       "      <td>...</td>\n",
       "      <td>149.656000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7415 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close   Adj Close  \\\n",
       "Datetime                                                                 \n",
       "1993-05-21    0.524554    0.527902    0.506696    0.513393    0.423102   \n",
       "1993-05-24    0.506696    0.524554    0.506696    0.514509    0.424022   \n",
       "1993-05-25    0.506696    0.513393    0.497768    0.503348    0.414824   \n",
       "1993-05-26    0.500000    0.515625    0.494420    0.515625    0.424942   \n",
       "1993-05-27    0.515625    0.522321    0.511161    0.513393    0.423102   \n",
       "...                ...         ...         ...         ...         ...   \n",
       "2022-10-24  147.190002  150.229996  146.000000  149.449997  149.449997   \n",
       "2022-10-25  150.089996  152.490005  149.360001  152.339996  152.339996   \n",
       "2022-10-25  150.089996  152.490005  149.360001  152.339996  152.339996   \n",
       "2022-10-26  150.960007  151.990005  148.039993  149.350006  149.350006   \n",
       "2022-10-27  148.070007  149.050003  144.130005  144.800003  144.800003   \n",
       "\n",
       "                 Volume        rsi  macd   en_center       en_ub  ...  \\\n",
       "Datetime                                                          ...   \n",
       "1993-05-21  148198400.0  64.511646  0.01    0.386439    0.405761  ...   \n",
       "1993-05-24  150315200.0  64.877338  0.01    0.386642    0.405974  ...   \n",
       "1993-05-25  180723200.0  58.396195  0.01    0.386550    0.405877  ...   \n",
       "1993-05-26  121564800.0  62.798638  0.01    0.386734    0.406071  ...   \n",
       "1993-05-27  197288000.0  61.523974  0.01    0.387083    0.406437  ...   \n",
       "...                 ...        ...   ...         ...         ...  ...   \n",
       "2022-10-24   75981900.0  54.436786 -1.88  153.653999  161.336699  ...   \n",
       "2022-10-25   74732290.0  58.207938 -1.12  153.236999  160.898849  ...   \n",
       "2022-10-25   74732300.0  58.207938 -0.51  152.823199  160.464359  ...   \n",
       "2022-10-26   88194300.0  52.949432 -0.27  152.319199  159.935159  ...   \n",
       "2022-10-27  108284100.0  46.121214 -0.44  151.732199  159.318809  ...   \n",
       "\n",
       "                SMA(5)   s1   s2   s3   s4  s5  s6   s7   s8   s9  \n",
       "Datetime                                                           \n",
       "1993-05-21    0.419055  0.0  1.0  0.0  1.0   0   0  1.0  1.0  0.0  \n",
       "1993-05-24    0.421815  0.0  1.0  0.0  0.0   0   0  1.0  0.0  0.0  \n",
       "1993-05-25    0.423102  0.0  1.0  0.0  1.0   0   0  1.0  0.0  0.0  \n",
       "1993-05-26    0.423838  0.0  1.0  0.0  0.0   0   0  1.0  1.0  0.0  \n",
       "1993-05-27    0.421998  0.0  1.0  0.0  0.0   0   0  1.0  1.0  0.0  \n",
       "...                ...  ...  ...  ...  ...  ..  ..  ...  ...  ...  \n",
       "2022-10-24  145.544000  0.0  0.0  0.0  1.0   0   1  0.0  1.0  0.0  \n",
       "2022-10-25  147.262000  0.0  0.0  0.0  0.0   0   1  1.0  1.0  0.0  \n",
       "2022-10-25  148.957999  0.0  1.0  0.0  0.0   0   1  0.0  0.0  0.0  \n",
       "2022-10-26  150.150000  0.0  1.0  0.0  0.0   0   0  0.0  0.0  0.0  \n",
       "2022-10-27  149.656000  0.0  1.0  0.0  0.0   0   0  0.0  0.0  0.0  \n",
       "\n",
       "[7415 rows x 29 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aapl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5927, 5, 29]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[5927, 5, 29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "class windowDataset(Dataset):\n",
    "    def __init__(self, data, target, input_window=5, output_window=1, stride=1):\n",
    "        L = data.shape[0]\n",
    "        num_samples = (L - input_window - output_window) // stride + 1\n",
    "\n",
    "        X = [] \n",
    "        y = [] \n",
    "\n",
    "        for i in range(input_window, len(data.values) - 1):\n",
    "            X.append(data.values[i-stride:i,:])\n",
    "            y.append(data[target].values[i:i+stride])\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        X = X.reshape(X.shape[0], X.shape[1], X.shape[2])#.transpose((1,0,2))\n",
    "        y = y.reshape(y.shape[0],y.shape[1], 1 )#.transpose((1,0,2))\n",
    "\n",
    "        self.x = X\n",
    "        self.y = y\n",
    "        \n",
    "        self.len = len(X)\n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = windowDataset(aapl, 'Adj Close')\n",
    "train_loader = DataLoader(train_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFModel(nn.Module):\n",
    "    def __init__(self,iw, ow, d_model, nhead, nlayers, dropout=0.5):\n",
    "        super(TFModel, self).__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=nlayers) \n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(29, d_model//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model//2, d_model)\n",
    "        )\n",
    "        \n",
    "        self.linear =  nn.Sequential(\n",
    "            nn.Linear(d_model, d_model//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model//2, 1)\n",
    "        )\n",
    "\n",
    "        self.linear2 = nn.Sequential(\n",
    "            nn.Linear(iw, (iw+ow)//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear((iw+ow)//2, ow)\n",
    "        ) \n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, srcmask):\n",
    "        # print(src.shape)\n",
    "        src = self.encoder(src)\n",
    "        # print(src.shape)\n",
    "        src = self.pos_encoder(src)\n",
    "        # print(src.shape)\n",
    "        output = self.transformer_encoder(src.transpose(0,1), srcmask).transpose(0,1)\n",
    "        # print(output.shape)\n",
    "        output = self.linear(output)[:,:,0]\n",
    "        # print(output.shape)\n",
    "        # output = self.linear2(output)\n",
    "        # print(output.shape)\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "def gen_attention_mask(x):\n",
    "    mask = torch.eq(x, 0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 29])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1575.137258:  13%|█▎        | 66/500 [08:00<52:37,  7.27s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [20], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     14\u001b[0m src_mask \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate_square_subsequent_mask(inputs\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 15\u001b[0m result \u001b[39m=\u001b[39m model(inputs\u001b[39m.\u001b[39;49mfloat()\u001b[39m.\u001b[39;49mto(device),  src_mask)\n\u001b[1;32m     16\u001b[0m loss \u001b[39m=\u001b[39m criterion(result, outputs[:,:,\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device))\n\u001b[1;32m     17\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [15], line 37\u001b[0m, in \u001b[0;36mTFModel.forward\u001b[0;34m(self, src, srcmask)\u001b[0m\n\u001b[1;32m     35\u001b[0m src \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_encoder(src)\n\u001b[1;32m     36\u001b[0m \u001b[39m# print(src.shape)\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer_encoder(src\u001b[39m.\u001b[39;49mtranspose(\u001b[39m0\u001b[39;49m,\u001b[39m1\u001b[39;49m), srcmask)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[39m# print(output.shape)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(output)[:,:,\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages/torch/nn/modules/transformer.py:280\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    277\u001b[0m         src_key_padding_mask_for_layers \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> 280\u001b[0m     output \u001b[39m=\u001b[39m mod(output, src_mask\u001b[39m=\u001b[39;49mmask, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask_for_layers)\n\u001b[1;32m    282\u001b[0m \u001b[39mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    283\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mto_padded_tensor(\u001b[39m0.\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages/torch/nn/modules/transformer.py:554\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    552\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x))\n\u001b[1;32m    553\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 554\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sa_block(x, src_mask, src_key_padding_mask))\n\u001b[1;32m    555\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n\u001b[1;32m    557\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages/torch/nn/modules/transformer.py:562\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sa_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor,\n\u001b[1;32m    561\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 562\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x,\n\u001b[1;32m    563\u001b[0m                        attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m    564\u001b[0m                        key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m    565\u001b[0m                        need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    566\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages/torch/nn/modules/activation.py:1176\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[1;32m   1165\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1166\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[1;32m   1167\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1173\u001b[0m         q_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj_weight, k_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj_weight,\n\u001b[1;32m   1174\u001b[0m         v_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj_weight, average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights)\n\u001b[1;32m   1175\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m   1177\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m   1178\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[1;32m   1179\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[1;32m   1180\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m   1181\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m   1182\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask, need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[1;32m   1183\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask, average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights)\n\u001b[1;32m   1184\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[1;32m   1185\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "lr = 1e-4\n",
    "model = TFModel(2, 5, 512, 8, 4, 0.1).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "epoch = 500\n",
    "model.train()\n",
    "progress = tqdm(range(epoch))\n",
    "for i in progress:\n",
    "    batchloss = 0.0\n",
    "    for (inputs, outputs) in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        src_mask = model.generate_square_subsequent_mask(inputs.shape[1]).to(device)\n",
    "        result = model(inputs.float().to(device),  src_mask)\n",
    "        loss = criterion(result, outputs[:,:,0].float().to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batchloss += loss\n",
    "    progress.set_description(\"loss: {:0.6f}\".format(batchloss.cpu().item() / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('dl_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b7f90f3ffd1b2a60fee91aee39c73355e94a75c9e00035a66944cc7ed4c1221"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
